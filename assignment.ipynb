{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: \n",
    "1. **Read Shakespeare's Works:**\n",
    "- Load a text file containing Shakespeare's works.\n",
    "- Preprocess the text by converting it to lowercase, removing punctuation, and splitting it into tokens.\n",
    "- Create a list of bigrams from the preprocessed text.\n",
    "\n",
    "2. **Dictionary of Bigram Counts:**\n",
    "- Write the code that fills the from_bigram_to_next_token_counts dictionary, where each key is a bigram (tuple of two tokens) and the value is a dictionary of counts of tokens that follow the bigram.\n",
    "- Example: from_bigram_to_next_token_counts[('to', 'be')] = {'or': 10, 'not': 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in Caesar:  25833\n",
      "Number of words in Hamlet:  37360\n",
      "Number of words in Macbeth:  23140\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.corpus.gutenberg.fileids()\n",
    "\n",
    "caesar = nltk.corpus.gutenberg.words('shakespeare-caesar.txt')\n",
    "hamlet = nltk.corpus.gutenberg.words('shakespeare-hamlet.txt')\n",
    "macbeth = nltk.corpus.gutenberg.words('shakespeare-macbeth.txt')\n",
    "\n",
    "print(\"Number of words in Caesar: \", len(caesar))\n",
    "print(\"Number of words in Hamlet: \", len(hamlet))\n",
    "print(\"Number of words in Macbeth: \", len(macbeth))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    tokens = [word.lower() for word in text if word.isalpha()]\n",
    "    return tokens\n",
    "\n",
    "caesar = preprocess(caesar)\n",
    "hamlet = preprocess(hamlet)\n",
    "macbeth = preprocess(macbeth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test for preprocess function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_basic (__main__.TestPreprocessCell1) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.002s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "def preprocess(tokens):\n",
    "    return [token.lower() for token in tokens if token.isalnum()]\n",
    "\n",
    "# Each cell gets its own test class with unique name\n",
    "class TestPreprocessCell1(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        # Reset any global state before each test\n",
    "        pass\n",
    "        \n",
    "    def test_basic(self):\n",
    "        self.assertEqual(\n",
    "            preprocess(['This', 'is', 'a', 'test', '.']),\n",
    "            ['this', 'is', 'a', 'test']\n",
    "        )\n",
    "\n",
    "# Run only tests defined in this cell\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(TestPreprocessCell1)\n",
    "unittest.TextTestRunner(verbosity=2).run(suite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 'tragedie'),\n",
       " ('tragedie', 'of'),\n",
       " ('of', 'julius'),\n",
       " ('julius', 'caesar'),\n",
       " ('caesar', 'by'),\n",
       " ('by', 'william'),\n",
       " ('william', 'shakespeare'),\n",
       " ('shakespeare', 'actus'),\n",
       " ('actus', 'primus'),\n",
       " ('primus', 'scoena')]"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caesar_bigrams = list(nltk.bigrams(caesar))\n",
    "hamlet_bigrams = list(nltk.bigrams(hamlet))\n",
    "macbeth_bigrams = list(nltk.bigrams(macbeth))\n",
    "\n",
    "caesar_bigrams[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'in': 3, 'fear': 1, 'taken': 1, 'he': 1, 'exalted': 1, 'afear': 1, 'render': 1, 'thus': 2, 'so': 1, 'resolu': 1, 'sent': 1, 'quarter': 1, 'but': 1, 'done': 3, 'led': 1, 'contracted': 1, 'disioynt': 1, 'neere': 1, 'commanded': 1, 'a': 4, 'nothing': 1, 'honest': 1, 'sounded': 1, 'or': 1, 'that': 2, 'wish': 1, 'considered': 1, 'fore': 1, 'free': 1, 'too': 1, 'blest': 1, 'kinde': 1, 'demanded': 1, 'last': 1, 'spilt': 1, 'your': 1, 'heard': 1, 'made': 2, 'buried': 1, 'damn': 1, 'king': 1, 'cawdor': 1, 'his': 1, 'the': 2, 'more': 1, 'watchers': 1, 'an': 1, 'trusted': 1, 'inuested': 1, 'safely': 1, 'wrencht': 1, 'giuen': 1, 'baited': 1}\n"
     ]
    }
   ],
   "source": [
    "from_bigram_to_next_token_counts = {}\n",
    "\n",
    "def update_counts_from_text(bigrams, tokens):\n",
    "    # Convert tokens to list for indexing\n",
    "    tokens = list(tokens)\n",
    "    \n",
    "    # For each bigram position\n",
    "    for i in range(len(bigrams)):\n",
    "        current_bigram = bigrams[i]\n",
    "        \n",
    "        # Get next token if not at end\n",
    "        if i < len(tokens) - 2:\n",
    "            next_token = tokens[i + 2]\n",
    "            \n",
    "            # Initialize nested dict if needed\n",
    "            if current_bigram not in from_bigram_to_next_token_counts:\n",
    "                from_bigram_to_next_token_counts[current_bigram] = {}\n",
    "                \n",
    "            # Update count for next token\n",
    "            next_token_dict = from_bigram_to_next_token_counts[current_bigram]\n",
    "            next_token_dict[next_token] = next_token_dict.get(next_token, 0) + 1\n",
    "\n",
    "update_counts_from_text(caesar_bigrams, caesar)\n",
    "update_counts_from_text(hamlet_bigrams, hamlet)\n",
    "update_counts_from_text(macbeth_bigrams, macbeth)\n",
    "print(from_bigram_to_next_token_counts[('to', 'be')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2:\n",
    "1. **Calculate Probabilities**\n",
    "- Write the code that fills the from_bigram_to_next_token_probs dictionary, where each key is a bigram and the value is a dictionary of probabilities of tokens that follow the bigram.\n",
    "- Use the counts from from_bigram_to_next_token_counts to calculate the probabilities.\n",
    "- Example: from_bigram_to_next_token_probs[('to', 'be')] = {'or': 0.67, 'not': 0.33}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'in': 0.046875, 'fear': 0.015625, 'taken': 0.015625, 'he': 0.015625, 'exalted': 0.015625, 'afear': 0.015625, 'render': 0.015625, 'thus': 0.03125, 'so': 0.015625, 'resolu': 0.015625, 'sent': 0.015625, 'quarter': 0.015625, 'but': 0.015625, 'done': 0.046875, 'led': 0.015625, 'contracted': 0.015625, 'disioynt': 0.015625, 'neere': 0.015625, 'commanded': 0.015625, 'a': 0.0625, 'nothing': 0.015625, 'honest': 0.015625, 'sounded': 0.015625, 'or': 0.015625, 'that': 0.03125, 'wish': 0.015625, 'considered': 0.015625, 'fore': 0.015625, 'free': 0.015625, 'too': 0.015625, 'blest': 0.015625, 'kinde': 0.015625, 'demanded': 0.015625, 'last': 0.015625, 'spilt': 0.015625, 'your': 0.015625, 'heard': 0.015625, 'made': 0.03125, 'buried': 0.015625, 'damn': 0.015625, 'king': 0.015625, 'cawdor': 0.015625, 'his': 0.015625, 'the': 0.03125, 'more': 0.015625, 'watchers': 0.015625, 'an': 0.015625, 'trusted': 0.015625, 'inuested': 0.015625, 'safely': 0.015625, 'wrencht': 0.015625, 'giuen': 0.015625, 'baited': 0.015625}\n"
     ]
    }
   ],
   "source": [
    "from_bigram_to_next_token_probs = {}\n",
    "update_counts_from_text(caesar_bigrams, caesar)\n",
    "update_counts_from_text(hamlet_bigrams, hamlet)\n",
    "update_counts_from_text(macbeth_bigrams, macbeth)\n",
    "\n",
    "from_bigram_to_next_token_probs = {}\n",
    "\n",
    "def convert_counts_to_probs():\n",
    "    for bigram, next_tokens in from_bigram_to_next_token_counts.items():\n",
    "        total_count = sum(next_tokens.values())\n",
    "        next_token_probs = {next_token: count / total_count for next_token, count in next_tokens.items()}\n",
    "        from_bigram_to_next_token_probs[bigram] = next_token_probs\n",
    "        \n",
    "convert_counts_to_probs()\n",
    "print(from_bigram_to_next_token_probs[('to', 'be')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3:\n",
    "1. **Implement a Sampling Function**\n",
    "- Implement the sample_next_token function, which should return the next token sampled based on the probability distribution from from_bigram_to_next_token_probs.\n",
    "- Ensure the sampling is done using a weighted random choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_bigram_to_next_token_counts = {}\n",
    "from_bigram_to_next_token_probs = {}\n",
    "\n",
    "update_counts_from_text(caesar_bigrams, caesar)\n",
    "update_counts_from_text(hamlet_bigrams, hamlet)\n",
    "update_counts_from_text(macbeth_bigrams, macbeth)\n",
    "\n",
    "convert_counts_to_probs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'your'"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def sample_next_token(bigram):\n",
    "    next_token_probs = from_bigram_to_next_token_probs[bigram]\n",
    "    return random.choices(list(next_token_probs.keys()), next_token_probs.values())[0]\n",
    "\n",
    "sample_next_token(('to', 'be'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4:\n",
    "1. **Generate Text**\n",
    "- Implement the generate_text_from_bigram function, which generates text by starting with an initial bigram and sampling the next token iteratively.\n",
    "- The function should return a string of generated text with a specified number of words.\n",
    "- Example: generate_text_from_bigram(('to', 'be'), 50) might return \"to be or not to be that is the question ...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to be neere me that are not wood you are come hither sirrah in parthia did i care not cass when caesar sayes do this in at his warning whether in sea or fire in earth or ayre th extrauagant and erring spirit hyes to his will and so i shall liue\n"
     ]
    }
   ],
   "source": [
    "def generate_text_from_bigram(bigram, num_tokens=50):\n",
    "    tokens = list(bigram)\n",
    "    for _ in range(num_tokens):\n",
    "        current_bigram = (tokens[-2], tokens[-1])\n",
    "        if current_bigram in from_bigram_to_next_token_probs:\n",
    "            tokens.append(sample_next_token(current_bigram))\n",
    "        else:\n",
    "            break  # Stop if the bigram is not found\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "print(generate_text_from_bigram(('to', 'be'), 50))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5:\n",
    "1. **Experiment with Trigrams and Quadgrams:**\n",
    "- Extend the model to work with trigrams (3-grams) and quadgrams (4-grams).\n",
    "- Implement similar functions for these n-grams: from_trigram_to_next_token_counts, from_trigram_to_next_token_probs, from_quadgram_to_next_token_counts, and from_quadgram_to_next_token_probs.\n",
    "- Analyze the differences in text generation quality between bigrams, trigrams, and quadgrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "caesar_trigrams = list(nltk.trigrams(caesar))\n",
    "hamlet_trigrams = list(nltk.trigrams(hamlet))\n",
    "macbeth_trigrams = list(nltk.trigrams(macbeth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to be or not to be that is the question whether tis nobler in the minde to lye in death mark antony shall say i am not gamesom i do lacke some part of that quicke spirit that is in antony let me not thinke on t frailty thy name is woman a\n"
     ]
    }
   ],
   "source": [
    "from_ngram_to_next_token_counts = {}\n",
    "from_ngram_to_next_token_probs = {}\n",
    "\n",
    "def update_counts_from_text_ngrams(ngrams, tokens): # same function, only it takes ngrams instead of bigrams\n",
    "    # Convert tokens to list for indexing\n",
    "    tokens = list(tokens)\n",
    "    \n",
    "    # For each ngram position\n",
    "    for i in range(len(ngrams)): \n",
    "        current_ngram = ngrams[i]\n",
    "        \n",
    "        # Get next token if not at end\n",
    "        if i < len(tokens) - len(current_ngram):\n",
    "            next_token = tokens[i + len(current_ngram)]\n",
    "            \n",
    "            # Initialize nested dict if needed\n",
    "            if current_ngram not in from_ngram_to_next_token_counts:\n",
    "                from_ngram_to_next_token_counts[current_ngram] = {}\n",
    "                \n",
    "            # Update count for next token\n",
    "            next_token_dict = from_ngram_to_next_token_counts[current_ngram]\n",
    "            next_token_dict[next_token] = next_token_dict.get(next_token, 0) + 1\n",
    "\n",
    "update_counts_from_text_ngrams(caesar_trigrams, caesar)\n",
    "update_counts_from_text_ngrams(hamlet_trigrams, hamlet)\n",
    "update_counts_from_text_ngrams(macbeth_trigrams, macbeth)\n",
    "\n",
    "def convert_counts_to_probs_ngrams():\n",
    "    for ngram, next_tokens in from_ngram_to_next_token_counts.items(): # same function for the bigrams but with the ngrams dictionary\n",
    "        total_count = sum(next_tokens.values())\n",
    "        next_token_probs = {next_token: count / total_count for next_token, count in next_tokens.items()}\n",
    "        from_ngram_to_next_token_probs[ngram] = next_token_probs\n",
    "        \n",
    "convert_counts_to_probs_ngrams()\n",
    "\n",
    "def sample_next_token_ngrams(ngram): # same as the original sample funcition but takes the ngram dictionary\n",
    "    next_token_probs = from_ngram_to_next_token_probs[ngram]\n",
    "    return random.choices(list(next_token_probs.keys()), next_token_probs.values())[0]\n",
    "\n",
    "def generate_text_from_ngram(ngram, num_tokens=50):\n",
    "    tokens = list(ngram)\n",
    "    for _ in range(num_tokens):\n",
    "        current_ngram = tuple(tokens[-len(ngram):])\n",
    "        if current_ngram in from_ngram_to_next_token_probs:\n",
    "            tokens.append(sample_next_token_ngrams(current_ngram))\n",
    "        else:\n",
    "            break  # Stop if ngram is not found\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "print(generate_text_from_ngram(('to', 'be', 'or'), 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to be or not to be that is the question whether tis nobler in the minde to suffer the slings and arrowes of outragious fortune or to take armes against a sea of troubles and by opposing end them to dye to sleepe no more and by a sleepe to say we end the\n"
     ]
    }
   ],
   "source": [
    "# quadrigrams\n",
    "\n",
    "# initialize the dictionaries\n",
    "from_ngram_to_next_token_counts = {}\n",
    "from_ngram_to_next_token_probs = {}\n",
    "\n",
    "caesar_quadrigrams = list(nltk.ngrams(caesar, 4))\n",
    "hamlet_quadrigrams = list(nltk.ngrams(hamlet, 4))\n",
    "macbeth_quadrigrams = list(nltk.ngrams(macbeth, 4))\n",
    "\n",
    "from_ngrams_to_next_token_counts = {}\n",
    "from_ngrams_to_next_token_probs = {}\n",
    "\n",
    "# add the quadrigrams from all the texts to the dict\n",
    "update_counts_from_text_ngrams(caesar_quadrigrams, caesar)\n",
    "update_counts_from_text_ngrams(hamlet_quadrigrams, hamlet)\n",
    "update_counts_from_text_ngrams(macbeth_quadrigrams, macbeth)\n",
    "\n",
    "convert_counts_to_probs_ngrams()\n",
    "\n",
    "print(generate_text_from_ngram(('to', 'be', 'or', 'not'), 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests\n",
    "\n",
    "My functions for the bigrams and n-grams are the same. The only thing that changes is the dictionary they use. Since i am using global dictionaries and perform inplace operacions for the counts and probabilities of bigrams and ngrams, the only thing that needs to change inside the functions (regardless of the function name) is the dictionary name. The tests will be the same for the functions with ngrams or bigrams. Therefore, to avoid redundancy, the tests below are performed on the n-gram functions only, since they work for the bigrams as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_basic (__main__.TestPreprocessCell1) ... ok\n",
      "test_empty (__main__.TestPreprocessCell1) ... ok\n",
      "test_no_punctuation (__main__.TestPreprocessCell1) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.006s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=3 errors=0 failures=0>"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "def preprocess(tokens):\n",
    "    return [token.lower() for token in tokens if token.isalnum()]\n",
    "\n",
    "# Each cell gets its own test class with unique name\n",
    "class TestPreprocessCell1(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        # Reset any global state before each test\n",
    "        pass\n",
    "        \n",
    "    def test_basic(self):\n",
    "        self.assertEqual(\n",
    "            preprocess(['This', 'is', 'a', 'test', '.']),\n",
    "            ['this', 'is', 'a', 'test']\n",
    "        )\n",
    "    \n",
    "    def test_no_punctuation(self):\n",
    "        self.assertEqual(\n",
    "            preprocess(['This', 'is', 'a', 'test']),\n",
    "            ['this', 'is', 'a', 'test']\n",
    "        )\n",
    "    \n",
    "    def test_empty(self):\n",
    "        self.assertEqual(\n",
    "            preprocess([]),\n",
    "            []\n",
    "        )\n",
    "    \n",
    "\n",
    "# Run only tests defined in this cell\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(TestPreprocessCell1)\n",
    "unittest.TextTestRunner(verbosity=2).run(suite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test (__main__.TestUpdateCountsCell1) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.003s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def update_counts_from_text_ngrams(ngrams, tokens):\n",
    "    # Convert tokens to list for indexing\n",
    "    tokens = list(tokens)\n",
    "    \n",
    "    # For each ngram position\n",
    "    for i in range(len(ngrams)): \n",
    "        current_ngram = ngrams[i]\n",
    "        \n",
    "        # Get next token if not at end\n",
    "        if i < len(tokens) - len(current_ngram):\n",
    "            next_token = tokens[i + len(current_ngram)]\n",
    "            \n",
    "            # Initialize nested dict if needed\n",
    "            if current_ngram not in from_ngram_to_next_token_counts:\n",
    "                from_ngram_to_next_token_counts[current_ngram] = {}\n",
    "                \n",
    "            # Update count for next token\n",
    "            next_token_dict = from_ngram_to_next_token_counts[current_ngram]\n",
    "            next_token_dict[next_token] = next_token_dict.get(next_token, 0) + 1\n",
    "\n",
    "class TestUpdateCountsCell1(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        # Reset global variable before each test\n",
    "        global from_ngram_to_next_token_counts\n",
    "        from_ngram_to_next_token_counts = {}\n",
    "        \n",
    "    def test(self):\n",
    "        ngrams = [('a', 'b'), ('b', 'c'), ('c', 'd')]\n",
    "        tokens = ['a', 'b', 'c', 'd']\n",
    "        update_counts_from_text_ngrams(ngrams, tokens)\n",
    "        self.assertEqual(\n",
    "            from_ngram_to_next_token_counts,\n",
    "            {('a', 'b'): {'c': 1}, ('b', 'c'): {'d': 1}}\n",
    "        )\n",
    "        self.assertEqual(\n",
    "            from_ngram_to_next_token_counts[('a', 'b')]['c'],\n",
    "            1\n",
    "        )\n",
    "        self.assertEqual(\n",
    "            from_ngram_to_next_token_counts[('b', 'c')]['d'],\n",
    "            1\n",
    "        ) \n",
    "\n",
    "# Run only tests from this cell\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(TestUpdateCountsCell1)\n",
    "unittest.TextTestRunner(verbosity=2).run(suite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_basic (__main__.TestProbabilityCell1) ... ok\n",
      "test_multiple (__main__.TestProbabilityCell1) ... ok\n",
      "test_nested (__main__.TestProbabilityCell1) ... ok\n",
      "test_repeated (__main__.TestProbabilityCell1) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.008s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=4 errors=0 failures=0>"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from_ngram_to_next_token_probs = {}\n",
    "\n",
    "def convert_counts_to_probs_ngrams():\n",
    "    for ngram, next_tokens in from_ngram_to_next_token_counts.items():\n",
    "        total_count = sum(next_tokens.values())\n",
    "        next_token_probs = {next_token: count / total_count for next_token, count in next_tokens.items()}\n",
    "        from_ngram_to_next_token_probs[ngram] = next_token_probs\n",
    "\n",
    "class TestProbabilityCell1(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        # Reset both global variables\n",
    "        global from_ngram_to_next_token_probs, from_ngram_to_next_token_counts\n",
    "        from_ngram_to_next_token_probs = {}\n",
    "        from_ngram_to_next_token_counts = {('a', 'b'): {'c': 1}}\n",
    "        \n",
    "    def test_basic(self):\n",
    "        convert_counts_to_probs_ngrams()\n",
    "        self.assertEqual(\n",
    "            from_ngram_to_next_token_probs,\n",
    "            {('a', 'b'): {'c': 1.0}}\n",
    "        )\n",
    "        \n",
    "    def test_multiple(self):\n",
    "        from_ngram_to_next_token_counts[('a', 'b')]['d'] = 2\n",
    "        convert_counts_to_probs_ngrams()\n",
    "        self.assertEqual(\n",
    "            from_ngram_to_next_token_probs,\n",
    "            {('a', 'b'): {'c': 0.3333333333333333, 'd': 0.6666666666666666}}\n",
    "        )\n",
    "        \n",
    "    def test_nested(self):\n",
    "        from_ngram_to_next_token_counts[('a', 'b')] = {}\n",
    "        convert_counts_to_probs_ngrams()\n",
    "        self.assertEqual(\n",
    "            from_ngram_to_next_token_probs,\n",
    "            {('a', 'b'): {}}\n",
    "        )\n",
    "        \n",
    "    def test_repeated(self):\n",
    "        from_ngram_to_next_token_counts[('a', 'b')] = {'c': 1}\n",
    "        convert_counts_to_probs_ngrams()\n",
    "        convert_counts_to_probs_ngrams()\n",
    "        self.assertEqual(\n",
    "            from_ngram_to_next_token_probs,\n",
    "            {('a', 'b'): {'c': 1.0}}\n",
    "        )\n",
    "\n",
    "# Run only tests from this cell\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(TestProbabilityCell1)\n",
    "unittest.TextTestRunner(verbosity=2).run(suite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test (__main__.TestSampleNextTokenCell1) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.002s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import unittest\n",
    "\n",
    "def sample_next_token_ngram(ngram):\n",
    "    next_token_probs = from_ngram_to_next_token_probs[ngram]\n",
    "    return random.choices(list(next_token_probs.keys()), next_token_probs.values())[0]\n",
    "\n",
    "\n",
    "class TestSampleNextTokenCell1(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        # Reset global variables\n",
    "        global from_ngram_to_next_token_probs\n",
    "        from_ngram_to_next_token_probs = {('a', 'b'): {'c': 1.0}}\n",
    "        \n",
    "    def test(self):\n",
    "        self.assertEqual(\n",
    "            sample_next_token_ngram(('a', 'b')),\n",
    "            'c'\n",
    "        )\n",
    "# Run only tests from this cell\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(TestSampleNextTokenCell1)\n",
    "unittest.TextTestRunner(verbosity=2).run(suite)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
